{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip -q install sagemaker awscli boto3 pandas --upgrade "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: PyTorch deployments using TorchServe and Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we’ll show you how you can build a TorchServe container and host it using Amazon SageMaker. With Amazon SageMaker hosting you get a fully-managed hosting experience. Just specify the type of instance, and the maximum and minimum number desired, and SageMaker takes care of the rest.\n",
    "\n",
    "With a few lines of code, you can ask Amazon SageMaker to launch the instances, download your model from Amazon S3 to your TorchServe container, and set up the secure HTTPS endpoint for your application. On the client side, get prediction with a simple API call to this secure endpoint backed by TorchServe.\n",
    "\n",
    "Code, configuration files, Jupyter notebooks and Dockerfiles used in this example are available here:\n",
    "https://github.com/shashankprasanna/torchserve-examples.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the TorchServe repository and install torch-model-archiver\n",
    "\n",
    "You'll use `torch-model-archiver` to create a model archive file. The .mar model archive file contains model checkpoints along with it’s `state_dict` (dictionary object that maps each layer to its parameter tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'serve' already exists and is not an empty directory.\n",
      "Processing ./serve/model-archiver\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/torchserve/lib/python3.7/site-packages (from torch-model-archiver==0.1.0b20200417) (0.18.2)\n",
      "Requirement already satisfied: enum-compat in /home/ec2-user/anaconda3/envs/torchserve/lib/python3.7/site-packages (from torch-model-archiver==0.1.0b20200417) (0.0.3)\n",
      "Building wheels for collected packages: torch-model-archiver\n",
      "  Building wheel for torch-model-archiver (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-model-archiver: filename=torch_model_archiver-0.1.0b20200417-py3-none-any.whl size=15843 sha256=a54b6d7e88c0942ee055859b9e512772e59f1521446475a7719657d21a7c62e8\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/19/d4/31/7884ea1291df5d92cf41bc1e2a78285b61474f93496a7e78af\n",
      "Successfully built torch-model-archiver\n",
      "Installing collected packages: torch-model-archiver\n",
      "  Attempting uninstall: torch-model-archiver\n",
      "    Found existing installation: torch-model-archiver 0.1.0b20200417\n",
      "    Uninstalling torch-model-archiver-0.1.0b20200417:\n",
      "      Successfully uninstalled torch-model-archiver-0.1.0b20200417\n",
      "Successfully installed torch-model-archiver-0.1.0b20200417\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pytorch/serve.git\n",
    "!pip install serve/model-archiver/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a PyTorch model and create a TorchServe archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR - /home/ec2-user/SageMaker/test/torchserve-examples/densenet161.mar already exists.\n",
      "Please specify --force/-f option to overwrite the model archive output file.\n",
      "See -h/--help for more details.\n",
      "densenet161.mar\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://download.pytorch.org/models/densenet161-8d451a50.pth\n",
    "    \n",
    "model_file_name = 'densenet161'\n",
    "\n",
    "!torch-model-archiver --model-name {model_file_name} \\\n",
    "--version 1.0 --model-file serve/examples/image_classifier/densenet_161/model.py \\\n",
    "--serialized-file densenet161-8d451a50.pth \\\n",
    "--extra-files serve/examples/image_classifier/index_to_name.json \\\n",
    "--handler image_classifier\n",
    "\n",
    "!ls *.mar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the generated densenet161.mar archive file to Amazon S3\n",
    "Choose a unique bucket name. The following lines of code will first create a bucket for you if bucket with the specified name doesn't already exist. Then it'll create a compressed tar.gz file our of the densenet161.mar file. Amazon SageMaker expects that models are in a tar.gz file. Finally it uploads the model to your S3 bucket under the models directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a boto3 session and get specify a role with SageMaker access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, time, json\n",
    "sess    = boto3.Session()\n",
    "sm      = sess.client('sagemaker')\n",
    "region  = sess.region_name\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densenet161.mar\n",
      "upload: ./densenet161.tar.gz to s3://sagemaker-us-west-2-453691756499/torchserve/model\n"
     ]
    }
   ],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "prefix = 'torchserve'\n",
    "\n",
    "!tar cvfz {model_file_name}.tar.gz densenet161.mar\n",
    "!aws s3 cp {model_file_name}.tar.gz s3://{bucket_name}/{prefix}/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Amazon ECR registry\n",
    "Create a new docker container registry for your torchserve container images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'torchserve' already exists in the registry with id '453691756499'\n"
     ]
    }
   ],
   "source": [
    "registry_name = 'torchserve'\n",
    "!aws ecr create-repository --repository-name {registry_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a TorchServe Docker container and push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  510.3MB\n",
      "Step 1/16 : FROM ubuntu:18.04\n",
      " ---> 4e5021d210f6\n",
      "Step 2/16 : ENV PYTHONUNBUFFERED TRUE\n",
      " ---> Using cache\n",
      " ---> 9606516f46f1\n",
      "Step 3/16 : RUN apt-get update &&     DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y     fakeroot     ca-certificates     dpkg-dev     g++     python3-dev     openjdk-11-jdk     curl     vim     && rm -rf /var/lib/apt/lists/*     && cd /tmp     && curl -O https://bootstrap.pypa.io/get-pip.py     && python3 get-pip.py\n",
      " ---> Using cache\n",
      " ---> f06b77639d7e\n",
      "Step 4/16 : RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
      " ---> Using cache\n",
      " ---> 90d7baec3c67\n",
      "Step 5/16 : RUN update-alternatives --install /usr/local/bin/pip pip /usr/local/bin/pip3 1\n",
      " ---> Using cache\n",
      " ---> b9ed1cdff896\n",
      "Step 6/16 : RUN pip install --no-cache-dir psutil                 --no-cache-dir torch                 --no-cache-dir torchvision\n",
      " ---> Using cache\n",
      " ---> 3a49828db3b7\n",
      "Step 7/16 : ADD serve serve\n",
      " ---> Using cache\n",
      " ---> a30051177634\n",
      "Step 8/16 : RUN pip install ../serve/\n",
      " ---> Using cache\n",
      " ---> a8b9c99cf43b\n",
      "Step 9/16 : COPY dockerd-entrypoint.sh /usr/local/bin/dockerd-entrypoint.sh\n",
      " ---> Using cache\n",
      " ---> f142dc7dcdc4\n",
      "Step 10/16 : RUN chmod +x /usr/local/bin/dockerd-entrypoint.sh\n",
      " ---> Using cache\n",
      " ---> 5678bfd24b26\n",
      "Step 11/16 : RUN mkdir -p /home/model-server/ && mkdir -p /home/model-server/tmp\n",
      " ---> Using cache\n",
      " ---> b72c5717000b\n",
      "Step 12/16 : COPY config.properties /home/model-server/config.properties\n",
      " ---> Using cache\n",
      " ---> 7adf958cc6c4\n",
      "Step 13/16 : WORKDIR /home/model-server\n",
      " ---> Using cache\n",
      " ---> 5ae0ca8b98a8\n",
      "Step 14/16 : ENV TEMP=/home/model-server/tmp\n",
      " ---> Using cache\n",
      " ---> a9da385e3fd8\n",
      "Step 15/16 : ENTRYPOINT [\"/usr/local/bin/dockerd-entrypoint.sh\"]\n",
      " ---> Using cache\n",
      " ---> 95e8e78f2065\n",
      "Step 16/16 : CMD [\"serve\"]\n",
      " ---> Using cache\n",
      " ---> 93e38ac0c332\n",
      "Successfully built 93e38ac0c332\n",
      "Successfully tagged torchserve:v1\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "The push refers to repository [453691756499.dkr.ecr.us-west-2.amazonaws.com/torchserve]\n",
      "\n",
      "\u001b[1B2734365e: Preparing \n",
      "\u001b[1Bddb3cfe9: Preparing \n",
      "\u001b[1B460d4b86: Preparing \n",
      "\u001b[1B89a6fb5a: Preparing \n",
      "\u001b[1Ba9d0e745: Preparing \n",
      "\u001b[1Bf0e0072d: Preparing \n",
      "\u001b[1B8a7a2cbf: Preparing \n",
      "\u001b[1B072619ea: Preparing \n",
      "\u001b[1B69ae95cb: Preparing \n",
      "\u001b[1B3c8a59c9: Preparing \n",
      "\u001b[1B2a8fc3be: Preparing \n",
      "\u001b[1Bda2e2e52: Preparing \n",
      "\u001b[1B83d4e999: Preparing \n",
      "\u001b[2B83d4e999: Layer already exists K\u001b[12A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[Kv1: digest: sha256:f7b6591d6ff9f0f32b5295732b6b4220618cde18d579980792e0ffefd43c763b size: 3247\n"
     ]
    }
   ],
   "source": [
    "image_label = 'v1'\n",
    "image = f'{account}.dkr.ecr.{region}.amazonaws.com/{registry_name}:{image_label}'\n",
    "\n",
    "!docker build -t {registry_name}:{image_label} .\n",
    "!$(aws ecr get-login --no-include-email --region {region})\n",
    "!docker tag {registry_name}:{image_label} {image}\n",
    "!docker push {image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy endpoint and make prediction using Amazon SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "\n",
    "model_data = f's3://{bucket_name}/{prefix}/models/{model_file_name}.tar.gz'\n",
    "sm_model_name = 'torchserve-densenet161'\n",
    "\n",
    "torchserve_model = Model(model_data = model_data, \n",
    "                         image = image,\n",
    "                         role  = role,\n",
    "                         predictor_cls=RealTimePredictor,\n",
    "                         name  = sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: torchserve-densenet161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'torchserve-endpoint-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "predictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',\n",
    "                                    initial_instance_count=1,\n",
    "                                    endpoint_name = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the TorchServe hosted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://s3.amazonaws.com/model-server/inputs/kitten.jpg    \n",
    "file_name = 'kitten.jpg'\n",
    "with open(file_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = payload\n",
    "    \n",
    "response = predictor.predict(data=payload)\n",
    "print(*json.loads(response), sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy endpoint and make prediction using Python SDK (Boto3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = f's3://{bucket_name}/{prefix}/models/{model_file_name}.tar.gz'\n",
    "sm_model_name = 'torchserve-densenet161-boto'\n",
    "\n",
    "container = {\n",
    "    'Image': image,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName         = sm_model_name,\n",
    "    ExecutionRoleArn  = role,\n",
    "    PrimaryContainer  = container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "endpoint_config_name = 'torchserve-endpoint-config-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "print(endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants = [{\n",
    "        'InstanceType'        : 'ml.m4.xlarge',\n",
    "        'InitialVariantWeight': 1,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName'           : sm_model_name,\n",
    "        'VariantName'         : 'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'torchserve-endpoint-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "print(endpoint_name)\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName         = endpoint_name,\n",
    "    EndpointConfigName   = endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/model-server/inputs/kitten.jpg    \n",
    "file_name = 'dog.jpg'\n",
    "with open(file_name, 'rb') as f:\n",
    "    payload = f.read()\n",
    "    payload = payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=payload)\n",
    "\n",
    "print(*json.loads(response['Body'].read()), sep = '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_torchserve",
   "language": "python",
   "name": "conda_torchserve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
